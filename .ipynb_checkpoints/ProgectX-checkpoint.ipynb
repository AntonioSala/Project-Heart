{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2e14dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4344821",
   "metadata": {},
   "source": [
    "# Importazione dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd72a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"heart_data.csv\")\n",
    "df.drop('id', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5ed26e",
   "metadata": {},
   "source": [
    "# Visualizzazione dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d386f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7080df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape #visualizziamo quante righe e colonne abbiamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e3c015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#controlliamo se ci sono righe duplicate\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5310889",
   "metadata": {},
   "outputs": [],
   "source": [
    "#controlliamo se ci sono valori nulli\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5c3cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verifichiamo che esistano solo i due valori binari 0 ed 1 che rappresentano rispettivamente: assenza o presenza di malattie cardiovascolari\n",
    "df['cardio'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf86b233",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3743086",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convertiamo da giorni ad anni\n",
    "ab=[]\n",
    "for x in df[\"age\"]:\n",
    "    x1=x//365\n",
    "    ab.append(x1)\n",
    "df.insert(1,\"Age\",ab)\n",
    "df.drop('age', axis = 1, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11ec46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T[1:]    #escludendo la conlonna dell'index andiamo ad ottenere vari dettagli sul nostro dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f62586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix to understand relation between variables\n",
    "\n",
    "plt.figure(figsize=(18, 8))\n",
    "sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True, cmap='vlag') # cmap='BrBG'\n",
    "plt.title('Correlation Map', fontdict={'fontsize':12}, pad=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4660927d",
   "metadata": {},
   "source": [
    "# Observations from the statistics description:\n",
    "\n",
    "minumum age recorded is 30 years old\n",
    "\n",
    "height(cm) and weight(kg) are objective features\n",
    "\n",
    "height - min: 55cm\n",
    "\n",
    "weight - min: 10kg\n",
    "\n",
    "ap_hi and ap_lo are examination features and both recorded extremes values\n",
    "\n",
    "ap_hi - min:-150 and max:16020\n",
    "\n",
    "ap_low - min:-70 and max:11000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664212d9",
   "metadata": {},
   "source": [
    "## Cerchiamo qualora ci fossero degli outliers:\n",
    "\n",
    "Tracciamo degli istogrammi per dare una controllata veloce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e912d33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Per tracciare gli istogrammi usiamo delle funzioni dalla library matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "\n",
    "colors = ['#960018', '#F1C40F'] # bordeaux e giallo oro della Roma\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.hist(df['height'], bins=50, color=colors[0])\n",
    "plt.title('height')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.hist(df['weight'], bins=50, color=colors[0])\n",
    "plt.title('weight')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.hist(df['ap_hi'], bins=30, color=colors[1])\n",
    "plt.title('ap_hi')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.hist(df['ap_lo'], bins=30, color=colors[1])\n",
    "plt.title('ap_lo')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# the extreme values in column ap_hi and ap_lo affected the range of x in historgrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d4ddbe",
   "metadata": {},
   "source": [
    "il codice crea una figura con 4 sottografici (subplot) con un numero di barre specificato (50 per altezza e peso e 30 per ap_hi e ap_lo). Ciascun sottografico presenta un titolo che indica la variabile rappresentata. Infine, il grafico viene visualizzato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa40efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rinominiamo le colonne per avere una chiara descrizione del contenuto del dataset\n",
    "df.rename(columns={'ap_hi': 'systolic', 'ap_lo': 'diastolic', 'gluc': 'glucose', 'alco': 'alcohol', 'cardio': 'cardiovascular disease', 'Age':'age'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a474b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed4f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Per verificare più nel dettaglio gli outliers di weight ed height nel nostro dataset usiamo una funzione dalla library di seaborn\n",
    "sns.lmplot(x='weight', y='height', hue='gender', data=df, fit_reg=True, height=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c57429",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c11c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove values more than 1.5 times the Inter Quartile Range (IQR) variable values\n",
    "def outliers_iqr(ys):\n",
    "    quartile1, quartile3 = np.percentile(ys, [25,75])\n",
    "    iqr = quartile3 - quartile1\n",
    "    lower_bound = quartile1 - (iqr*1.5)\n",
    "    upper_bound = quartile3 + (iqr*1.5)\n",
    "    \n",
    "    print(f'Q1:{quartile1}, Q3:{quartile3}, IQR:{iqr}')\n",
    "    print(f'Lower Bound:{lower_bound}, Upper Bound:{upper_bound}')\n",
    "    \n",
    "    result = np.where((ys > upper_bound) | (ys < lower_bound))\n",
    "    boundary = (lower_bound, upper_bound)\n",
    "    \n",
    "    print(f'Number of outliers: {len(result[0])}')\n",
    "    \n",
    "    return result, boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81596aba",
   "metadata": {},
   "source": [
    "Il codice definisce una funzione chiamata \"outliers_iqr\" che prende in input una serie di dati numerici \"ys\" e restituisce una tupla contenente un array con gli indici degli outlier e il valore di lower_bound e upper_bound, ovvero il limite inferiore e superiore dell'intervallo che definisce gli outlier.\n",
    "\n",
    "All'interno della funzione, la variabile \"quartile1\" rappresenta il primo quartile, \"quartile3\" rappresenta il terzo quartile e \"iqr\" rappresenta l'Inter Quartile Range (IQR), calcolato come la differenza tra il terzo e il primo quartile.\n",
    "\n",
    "Successivamente, viene calcolato il limite inferiore e superiore come quartile1 - (iqr3) e quartile3 + (iqr3), rispettivamente, che corrispondono a 1.5 volte l'IQR.\n",
    "\n",
    "Infine, vengono selezionati gli outlier utilizzando la funzione \"np.where\" che restituisce un array booleano indicando quali valori soddisfano la condizione specificata (ovvero che siano al di fuori degli intervalli definiti da lower_bound e upper_bound).\n",
    "\n",
    "La funzione restituisce l'array di indici degli outlier e i limiti dell'intervallo degli outlier.\n",
    "\n",
    "Nel resto del codice, viene utilizzata la funzione \"outliers_iqr\" per selezionare gli outlier della variabile \"height\" all'interno del dataframe \"df\". In particolare, gli indici degli outlier vengono salvati in una lista chiamata \"height_outlier_index\" e il dataframe contenente gli outlier viene salvato in \"df_height_outlier\".\n",
    "\n",
    "Inoltre, la funzione \"outliers_iqr\" stampa a video la Q1, Q3 e l'IQR per la variabile di input, i limiti inferiore e superiore dell'intervallo degli outlier e il numero di outlier trovati. Nell'esempio specifico, ci sono 93 outlier nella variabile \"height\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc75326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "height_outlier_index = list(outliers_iqr(df['height'])[0][0])\n",
    "df_height_outlier = df.iloc[height_outlier_index,:]\n",
    "df_height_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c5aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_outlier_index = list(outliers_iqr(df['weight'])[0][0])\n",
    "df_weight_outlier = df.iloc[weight_outlier_index,:]\n",
    "df_weight_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4eb971",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_hi_outlier_index = list(outliers_iqr(df['systolic'])[0][0])\n",
    "df_aphi_outlier = df.iloc[ap_hi_outlier_index,:]\n",
    "df_aphi_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd71df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_lo_outlier_index = list(outliers_iqr(df['diastolic'])[0][0])\n",
    "df_aplo_outlier = df.iloc[ap_lo_outlier_index,:]\n",
    "df_aplo_outlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e23f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove outliers\n",
    "all_outlier_index = height_outlier_index + weight_outlier_index + ap_hi_outlier_index + ap_lo_outlier_index\n",
    "all_outlier_index = list(set(all_outlier_index))\n",
    "print(len(all_outlier_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda08ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.drop(index = all_outlier_index).reset_index()\n",
    "df_clean.drop(columns=['index','level_0'], inplace=True)\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf2ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizziamo gli effetti della nostra \"pulitura\"\n",
    "for col in df_clean.columns:\n",
    "    print(col)\n",
    "    print(df_clean[col].value_counts())\n",
    "    print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03223f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0738a8eb",
   "metadata": {},
   "source": [
    "Il codice che segue utilizza la funzione countplot() della libreria Seaborn per creare un grafico a barre che mostra il numero di pazienti con e senza malattia cardiovascolare, suddivisi per genere.\n",
    "\n",
    "In particolare, l'argomento x='gender' indica che la variabile \"gender\" del DataFrame df sarà utilizzata come variabile sull'asse x del grafico. L'argomento data=df indica che i dati utilizzati per il grafico sono contenuti nel DataFrame df. L'argomento hue='cardiovascular disease' indica che la variabile \"cardiovascular disease\" del DataFrame df sarà utilizzata per suddividere i dati in due gruppi distinti (con e senza malattia cardiovascolare), che saranno rappresentati con colori diversi all'interno del grafico.\n",
    "\n",
    "L'output del grafico mostrerà due barre per ogni genere, una per i pazienti senza malattia cardiovascolare e una per i pazienti con malattia cardiovascolare. L'asse y del grafico rappresenterà il numero di pazienti in ciascun gruppo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95caeb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad75c8a1",
   "metadata": {},
   "source": [
    "Note: Il nostro dataset adesso sembra più realistico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a374b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bbd29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff70199",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0586375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aed4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba24a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bf5023",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_palette = {0:'#960018', 1:'#FFD200'}\n",
    "sns.countplot(x='gender', hue='cardiovascular disease', data=df_train, palette = my_palette)\n",
    "plt.xticks([0,1],['Male','Female'])\n",
    "plt.show()\n",
    "## Non sembra esserci differenza tra uomini e donne nella possibilità di sviluppare malattie cardiovascolari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7687614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='cholesterol', hue='cardiovascular disease', data=df_train, palette =my_palette)\n",
    "plt.show()\n",
    "# There appears to be a correlation between higher cholesterol levels and cardiovascular disease\n",
    "# chloesterol levels: 1 = normal, 2 = above normal, 3 = well above normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648ddeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='glucose', hue='cardiovascular disease', data=df_train, palette = my_palette)\n",
    "plt.show()\n",
    "# There appears to be another correlation between higher glucose levels and cardiovascular disease\n",
    "# glucose levels: 1 = normal, 2 = above normal, 3 = well above normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d60215",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='active', hue='cardiovascular disease', data=df_train, palette = my_palette)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02369f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='smoke', hue='cardiovascular disease', data=df_train, palette = my_palette)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87230391",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.countplot(x='alcohol', hue='cardiovascular disease', data=df_train, palette= my_palette)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e948ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.histplot(data=df_train, x='weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29437cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df_train, x='height')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8022fe95",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "\n",
    "Il \"Feature Engineering\" è una tecnica utilizzata nell'analisi dei dati per creare nuove variabili (chiamate \"feature\") che possano aiutare a migliorare le prestazioni dei modelli di machine learning.\n",
    "\n",
    "Il Body Mass Index (BMI) è una metrica comune utilizzata per la valutazione medica e la salute del cuore. Il BMI può essere calcolato nel seguente modo: BMI = peso(kg) / altezza (cm) / altezza (cm) x 10,000.\n",
    "\n",
    "La Pressione di Polso è un altro indicatore della salute del cuore. La Pressione di Polso può essere calcolata nel seguente modo: Pressione di Polso = sistolica - diastolica. Tipicamente, una pressione di polso superiore a 60 può essere un utile predittore di infarti o altre malattie cardiovascolari.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d889a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['BMI'] = df_train['weight'] / df_train['height'] / df_train['height'] * 10000\n",
    "df_train['pulse pressure'] = df_train['systolic'] - df_train['diastolic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30e360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()\n",
    "# Quick look at the dataframe to make sure these new features have been added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaadac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(df_train['BMI'], bins=50, kde=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6a57c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train['BMI'] > 45].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e69cdd7",
   "metadata": {},
   "source": [
    "Breve osservazione per verificare se i valori estremamente elevati di BMI sono correlati alle malattie cardiovascolari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027494de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[(df_train['pulse pressure'] >= 60 ) & (df_train['cholesterol'] == 3)].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8194dad",
   "metadata": {},
   "source": [
    "Sguardo superficiale alle persone che presentano sia un'alta pressione differenziale (>=60) che livelli di colesterolo ben al di sopra della norma (3).\n",
    "Dopo aver esaminato i primi diversi casi, risulta che avere sia un'alta pressione differenziale che livelli di colesterolo ben al di sopra della norma sia correlato ad una maggior probabilità di avere malattie cardiovascolari."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a5369c",
   "metadata": {},
   "source": [
    "# Machine learning\n",
    "### -SVM\n",
    "### -Naive bayes\n",
    "### -Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b4fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and testing datasets\n",
    "X = df_train.drop(['weight', 'height', 'cardiovascular disease'], axis=1)\n",
    "y = df_train['cardiovascular disease']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c38c6c",
   "metadata": {},
   "source": [
    "Questo codice crea un modello di machine learning per prevedere la presenza di malattie cardiovascolari in base ad alcune caratteristiche del paziente. In particolare, viene utilizzato un dataset chiamato 'df_train' che contiene alcune informazioni su un gruppo di pazienti, tra cui peso, altezza e la presenza o meno di malattie cardiovascolari.\n",
    "\n",
    "Per creare il modello, la prima cosa che viene fatta è rimuovere le colonne 'peso', 'altezza' e 'malattie cardiovascolari' dal dataset originale e assegnare i dati rimanenti alla variabile X. La variabile y viene invece creata per contenere i dati relativi alla presenza o meno di malattie cardiovascolari.\n",
    "\n",
    "Successivamente, viene utilizzata la funzione 'train_test_split' della libreria scikit-learn per suddividere i dati in un set di addestramento (X_train e y_train) e un set di test (X_test e y_test). La suddivisione viene fatta in modo casuale, impostando il parametro 'random_state' a 17 e specificando che il 30% dei dati venga utilizzato per il set di test (test_size=0.3).\n",
    "\n",
    "In questo modo, è possibile utilizzare i dati del set di addestramento per creare un modello di machine learning che possa prevedere la presenza di malattie cardiovascolari in base alle altre caratteristiche dei pazienti, e verificare l'accuratezza del modello utilizzando i dati del set di test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f0f105",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ab971f",
   "metadata": {},
   "source": [
    "Questo codice utilizza la classe StandardScaler della libreria scikit-learn per normalizzare i dati del set di addestramento (X_train) e del set di test (X_test).\n",
    "\n",
    "La standardizzazione dei dati è una pratica comune nel machine learning che consiste nel trasformare i dati in modo che abbiano una media pari a zero e una deviazione standard pari a uno. Questo è utile perché alcuni algoritmi di machine learning, come ad esempio le reti neurali, funzionano meglio se i dati sono normalizzati.\n",
    "\n",
    "Nel codice, viene prima creato un oggetto StandardScaler e assegnato alla variabile sc_X. Quindi, la funzione fit_transform viene utilizzata sul set di addestramento X_train per adattare lo scaler ai dati e normalizzarli contemporaneamente. La normalizzazione del set di test X_test viene invece effettuata con la funzione transform, che applica la stessa trasformazione ai dati di test utilizzando i parametri appresi durante la normalizzazione del set di addestramento.\n",
    "\n",
    "In questo modo, i dati del set di addestramento e del set di test sono normalizzati in modo coerente e possono essere utilizzati per addestrare e valutare un modello di machine learning in modo più accurato.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858736f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bed584",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a75f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6508a670",
   "metadata": {},
   "source": [
    "Questo codice addestra un classificatore di tipo Random Forest utilizzando la classe RandomForestClassifier della libreria scikit-learn.\n",
    "\n",
    "In particolare, viene creato un oggetto RandomForestClassifier e assegnato alla variabile rfc. Il parametro n_estimators viene impostato a 100, il che significa che verranno creati 100 alberi decisionali (o estimatori) per comporre l'ensemble del classificatore.\n",
    "\n",
    "Successivamente, il classificatore viene addestrato utilizzando il metodo fit, che prende come input il set di addestramento normalizzato X_train e il corrispondente vettore di etichette y_train.\n",
    "\n",
    "Il classificatore di tipo Random Forest è una tecnica di machine learning che si basa sull'insieme di più alberi decisionali, ognuno dei quali viene addestrato su un sottoinsieme casuale dei dati di addestramento. Alla fine, le previsioni dei singoli alberi vengono combinate utilizzando una maggioranza di voti per produrre la previsione finale del classificatore.\n",
    "\n",
    "In questo caso, il classificatore Random Forest viene utilizzato per prevedere la presenza o meno di malattie cardiovascolari nei pazienti, in base alle altre caratteristiche presenti nei dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f53893",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfc = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c60910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model Evaluation\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_rfc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3295a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_rfc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fae5e42",
   "metadata": {},
   "source": [
    "Questo codice stampa le metriche di valutazione del modello Random Forest addestrato in precedenza.\n",
    "\n",
    "In particolare, viene utilizzata la classe confusion_matrix della libreria scikit-learn per calcolare la matrice di confusione del modello. La matrice di confusione è una tabella che mostra il numero di previsioni corrette e incorrette del modello, suddivise per classe di output. In questo caso, la classe di output è rappresentata dalla presenza o meno di malattie cardiovascolari.\n",
    "\n",
    "Successivamente, viene utilizzata la classe classification_report della libreria scikit-learn per stampare un rapporto di classificazione del modello. Il rapporto di classificazione fornisce alcune delle principali metriche di valutazione del modello, tra cui l'accuratezza, la precisione, il richiamo (recall) e la F1-score. L'accuratezza è definita come il rapporto tra il numero di previsioni corrette e il numero totale di previsioni, mentre la precisione e il richiamo sono calcolati sulla base della matrice di confusione e misurano la capacità del modello di identificare correttamente le istanze positive e negative. La F1-score è una media armonica tra precisione e richiamo.\n",
    "\n",
    "Queste metriche possono fornire una valutazione della capacità del modello di effettuare previsioni accurate sulla base dei dati di test.\n",
    "La media armonica è una tipologia di media statistica che viene utilizzata per calcolare il valore medio di un insieme di numeri.\n",
    "\n",
    "A differenza della più comune media aritmetica, che viene calcolata sommando tutti i valori e dividendo per il numero totale di valori, la media armonica viene calcolata come il reciproco della media aritmetica dei reciproci dei valori. In altre parole, la media armonica di un insieme di numeri è data dal rapporto tra il numero totale di numeri e la somma dei reciproci di tali numeri.\n",
    "\n",
    "La media armonica è particolarmente utile quando si vogliono calcolare medie di grandezze che dipendono da rapporti, come ad esempio velocità, flussi o tassi di cambio. In questo caso, la media armonica viene utilizzata per calcolare la media di una grandezza che è inversamente proporzionale ai valori di input.\n",
    "\n",
    "Ad esempio, se si ha un'istanza positiva e un'istanza negativa in un problema di classificazione binaria, e il modello ha una precisione del 70% per le istanze positive e del 90% per le istanze negative, la media armonica delle precisioni sarà:\n",
    "\n",
    "2 / (1/0.7 + 1/0.9) = 0.788\n",
    "La media armonica è stata utilizzata anche come base per la definizione della popolare metrica di valutazione del recupero informazioni F1-score, che rappresenta la media armonica tra la precisione e il richiamo (recall) di un classificatore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847cceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47eee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying k-Fold Cross Validation\n",
    "\n",
    "accuracies_rfc = cross_val_score(estimator=rfc, X=X_train, y=y_train, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb22f80b",
   "metadata": {},
   "source": [
    "Il codice che hai mostrato esegue la tecnica di validazione incrociata a k-fold sul modello Random Forest creato in precedenza.\n",
    "\n",
    "La validazione incrociata a k-fold è una tecnica di valutazione del modello che consiste nel suddividere i dati in k parti uguali, addestrare il modello su k-1 parti e testarlo sulla parte rimanente. Questa operazione viene ripetuta k volte, in modo che ogni parte del dataset venga utilizzata esattamente una volta come set di test. In questo modo, si ottiene una stima più affidabile delle prestazioni del modello rispetto alla semplice suddivisione dei dati in set di training e di test.\n",
    "\n",
    "Nel codice mostrato, viene utilizzata la classe cross_val_score della libreria scikit-learn per eseguire la validazione incrociata a 10 fold sul modello Random Forest creato in precedenza. La funzione restituisce un array contenente l'accuratezza del modello per ogni fold. L'accuratezza è definita come il rapporto tra il numero di previsioni corrette e il numero totale di previsioni.\n",
    "\n",
    "In pratica, la tecnica di validazione incrociata a k-fold può aiutare a verificare se il modello sta effettivamente imparando la relazione tra le variabili di input e di output o se sta semplicemente imparando a memoria i dati di training. Inoltre, può fornire una stima più affidabile delle prestazioni del modello su dati non visti in precedenza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07439f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7fed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_rfc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2937795",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_rfc.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87a42c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {\n",
    "    'age': 40,\n",
    "    'gender': 1,\n",
    "    'height': 180,\n",
    "    'weight': 75.0,\n",
    "    'systolic': 130,\n",
    "    'diastolic': 80,\n",
    "    'cholesterol': 1,\n",
    "    'glucose': 1,\n",
    "    'smoke': 0,\n",
    "    'alcohol': 1,\n",
    "    'active': 1,\n",
    "    'BMI': 23.15,\n",
    "    'pulse pressure': 50\n",
    "}\n",
    "\n",
    "df_input = pd.DataFrame([new_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20491a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rimuovi le colonne di height e weight dal dataframe di input\n",
    "X_input = df_input.drop(['height', 'weight'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4a66a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizza le feature utilizzando il scaler già definito\n",
    "scaler = StandardScaler(with_mean=False, with_std=False)\n",
    "scaler.fit(X_train)\n",
    "X_input = scaler.transform(X_input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33328b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fai previsioni sul nuovo insieme di dati di input\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=17)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6c8ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# stampa la previsione\n",
    "print(y_pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ee2ce5",
   "metadata": {},
   "source": [
    "Se l'output di print(y_pred[0]) è 1, significa che il modello di Random Forest ha previsto che la persona descritta dai dati di input ha una malattia cardiovascolare. Al contrario, se l'output fosse stato 0, non sarebbe stata prevista una malattia cardiovascolare per quella persona.\n",
    "\n",
    "Va ricordato che il valore di previsione dipende dalla soglia di decisione (o di cutoff) che è stata scelta per il modello. Se la probabilità di appartenenza alla classe positiva (ovvero di avere una malattia cardiovascolare) calcolata dal modello supera la soglia di decisione, il modello prevede una malattia cardiovascolare. Altrimenti, prevede che non ci sia una malattia cardiovascolare. La soglia di decisione di default è di 0.5, ma può essere regolata a seconda delle esigenze specifiche del problema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41370f7f",
   "metadata": {},
   "source": [
    "**SVM Model Investigation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c132c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn import svm\n",
    "svc = svm.SVC(kernel='linear')\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fd9b6c",
   "metadata": {},
   "source": [
    "Questo codice utilizza la libreria Scikit-Learn (abbreviata spesso come sklearn) per creare una macchina a vettori di supporto (SVM, \"support vector machine\") con un kernel lineare. Questo significa che viene creato un modello che può essere utilizzato per classificare i dati in base a diverse feature, usando il principio di separazione degli iperpiani (SVM).\n",
    "\n",
    "La prima riga importa la classe svm dalla libreria sklearn, mentre la seconda riga crea un oggetto SVM con un kernel lineare. Il parametro kernel specifica il tipo di kernel da utilizzare, in questo caso lineare.\n",
    "\n",
    "Successivamente, il codice usa il metodo fit() dell'oggetto SVM per addestrare il modello sui dati di addestramento X_train e y_train. X_train contiene le feature utilizzate per addestrare il modello, mentre y_train contiene le corrispondenti etichette di classe associate a ciascuna osservazione. Il modello addestrato può quindi essere utilizzato per classificare nuovi dati.\n",
    "\n",
    "In generale, l'addestramento di un modello SVM comporta la definizione di un iperpiano che separa i dati di addestramento in modo ottimale, minimizzando la distanza tra l'iperpiano e i punti più vicini a esso. Nel caso del kernel lineare, l'iperpiano è definito da una retta. L'obiettivo dell'addestramento è quello di trovare i coefficienti di questa retta in modo che si possa classificare correttamente il maggior numero possibile di punti di addestramento.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Antonio Salanitro\n",
    "il kernel lineare per cosa viene usato nello specifico?\n",
    "Il kernel lineare è uno dei tipi di kernel che può essere utilizzato nelle macchine a vettori di supporto (SVM), un algoritmo di apprendimento supervisionato che viene utilizzato principalmente per la classificazione di dati.\n",
    "\n",
    "Il kernel lineare è usato per separare i dati di addestramento utilizzando un iperpiano lineare, che è definito come una retta in uno spazio bidimensionale o come un piano in uno spazio tridimensionale o superiore. L'iperpiano lineare viene posizionato in modo da separare i punti di una classe da quelli dell'altra classe.\n",
    "\n",
    "In particolare, l'utilizzo del kernel lineare in un SVM è indicato quando i dati di addestramento possono essere separati linearmente, cioè quando è possibile disegnare una linea retta o un piano nello spazio delle feature che separa perfettamente i dati delle diverse classi. In questo caso, l'iperpiano lineare può essere addestrato in modo da massimizzare la distanza tra i punti più vicini delle due classi, creando così un confine di decisione chiaro e ben definito.\n",
    "\n",
    "Il kernel lineare è spesso utilizzato come punto di partenza per la selezione del kernel migliore per un determinato problema di classificazione, in quanto è relativamente semplice da addestrare e da interpretare. Tuttavia, se i dati non sono linearmente separabili, è possibile che altri tipi di kernel, come il kernel polinomiale o il kernel gaussiano, possano produrre risultati migliori.\n",
    "(ovviamente li ottestati e non è così, il lineare è il più accurato in questo caso)\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "svc = svm.SVC(kernel='poly', degree=3) # kernel polinomiale di grado 3\n",
    "svc.fit(X_train, y_train) # addestramento del modello sui dati di addestramento\n",
    "In questo esempio, l'oggetto SVC viene creato con un kernel polinomiale di grado 3 (cioè un polinomio cubico), che verrà utilizzato per separare i dati di addestramento. Il modello viene quindi addestrato sui dati di addestramento X_train e y_train, utilizzando il metodo fit().\n",
    "\n",
    "## Il kernel polinomiale \n",
    "viene utilizzato quando i dati non possono essere separati linearmente, ma potrebbero essere separati utilizzando un confine di decisione curvo. Il grado del polinomio specifica il grado massimo del polinomio utilizzato per generare il confine di decisione. Un valore troppo alto del grado del polinomio può portare a un'eccessiva complessità del modello e al rischio di overfitting, mentre un valore troppo basso può rendere il modello insufficientemente flessibile per adattarsi ai dati di addestramento.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "svc = svm.SVC(kernel='rbf', gamma=0.1) # kernel gaussiano con gamma = 0.1\n",
    "svc.fit(X_train, y_train) # addestramento del modello sui dati di addestramento\n",
    "In questo esempio, l'oggetto SVC viene creato con un kernel gaussiano (noto anche come kernel RBF, \"radial basis function\") e il parametro gamma viene impostato a 0.1. Il parametro gamma controlla la larghezza della funzione gaussiana, che viene utilizzata per calcolare la somiglianza tra coppie di punti nel calcolo del confine di decisione.\n",
    "\n",
    "## Il kernel gaussiano\n",
    "viene utilizzato quando i dati non possono essere separati linearmente e un confine di decisione curvo non è sufficiente per separare le classi. In questo caso, il kernel gaussiano può creare una superficie di separazione curva e più flessibile rispetto a quella generata da un kernel polinomiale. Tuttavia, il parametro gamma controlla la flessibilità del modello e deve essere impostato con attenzione per evitare l'overfitting dei dati di addestramento.\n",
    "\n",
    "Il modello viene poi addestrato sui dati di addestramento X_train e y_train, utilizzando il metodo fit().\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06865547",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svc = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be31bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_svc) #questo l'ho messo perchè voglio fare vedere ad Auro l'array generato"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e198d0bb",
   "metadata": {},
   "source": [
    "Il codice y_pred_svc = svc.predict(X_test) esegue la previsione delle etichette di classe dei dati di test utilizzando il modello addestrato svc, memorizzando i risultati in una nuova variabile y_pred_svc.\n",
    "\n",
    "In particolare, il metodo predict() dell'oggetto SVC di Scikit-Learn prende come input i dati di test X_test e restituisce un array contenente le etichette di classe previste per ciascun campione dei dati di test.\n",
    "\n",
    "Di conseguenza, il codice y_pred_svc = svc.predict(X_test) assegna tali etichette previste alla variabile y_pred_svc, che può essere utilizzata per valutare le prestazioni del modello sui dati di test, ad esempio confrontando le etichette previste con quelle vere delle istanze di test tramite una metrica come l'accuratezza o la matrice di confusione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad276d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SVM Model Evaluation\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_svc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fb0cc4",
   "metadata": {},
   "source": [
    "Il codice print(confusion_matrix(y_test, y_pred_svc)) stampa la matrice di confusione del modello SVM addestrato sui dati di addestramento e valutato sui dati di test.\n",
    "\n",
    "La matrice di confusione è una tabella che mostra il numero di campioni correttamente e erroneamente classificati dal modello per ciascuna classe. In particolare, la matrice di confusione è composta da quattro valori: il numero di veri positivi (TP), il numero di falsi positivi (FP), il numero di falsi negativi (FN) e il numero di veri negativi (TN). Questi valori possono essere utilizzati per calcolare diverse metriche di valutazione del modello, come l'accuratezza, la precisione, la recall o la F1-score.\n",
    "\n",
    "Nel codice print(confusion_matrix(y_test, y_pred_svc)), i parametri y_test e y_pred_svc rappresentano rispettivamente le etichette vere e le etichette previste dal modello SVM sui dati di test. Il metodo confusion_matrix() di Scikit-Learn prende come input queste due variabili e restituisce la matrice di confusione del modello.\n",
    "\n",
    "La stampa della matrice di confusione può aiutare a capire le prestazioni del modello SVM sul dataset di test, ad esempio identificando se il modello ha difficoltà a classificare alcune classi in particolare o se mostra una buona capacità di generalizzazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c6851",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e7e02c",
   "metadata": {},
   "source": [
    "Il codice print(classification_report(y_test, y_pred_svc)) stampa un report di classificazione del modello SVM addestrato sui dati di addestramento e valutato sui dati di test.\n",
    "\n",
    "Il report di classificazione contiene alcune metriche di valutazione del modello, come la precisione, la recall, la F1-score e il supporto, per ciascuna classe. Queste metriche sono calcolate utilizzando le etichette vere e le etichette previste dal modello sui dati di test.\n",
    "\n",
    "In particolare, la precisione misura la frazione di istanze classificate come positive dal modello che sono effettivamente positive, la recall misura la frazione di istanze positive che sono correttamente identificate dal modello, la F1-score è una media armonica della precisione e della recall, e il supporto è il numero di istanze appartenenti a ciascuna classe.\n",
    "\n",
    "Nel codice print(classification_report(y_test, y_pred_svc)), i parametri y_test e y_pred_svc rappresentano rispettivamente le etichette vere e le etichette previste dal modello SVM sui dati di test. Il metodo classification_report() di Scikit-Learn prende come input queste due variabili e restituisce un report di classificazione del modello.\n",
    "\n",
    "La stampa del report di classificazione può aiutare a capire le prestazioni del modello SVM sul dataset di test, ad esempio identificando se il modello ha difficoltà a classificare alcune classi in particolare o se mostra una buona capacità di generalizzazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95fe60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e0af5c",
   "metadata": {},
   "source": [
    "**K-Fold cross-valuidation of SVM model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715b91c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying k-Fold Cross Validation\n",
    "accuracies_svc = cross_val_score(estimator=svc, X=X_train, y=y_train, cv=10, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732ab767",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7d272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_svc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea761eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_svc.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2ef38a",
   "metadata": {},
   "source": [
    "**Naive Bayes Model Investigation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c28951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nbc = GaussianNB()\n",
    "nbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26583bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nbc = nbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed592d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Model Evaluation\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_nbc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e37d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_nbc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b633e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca5fd04",
   "metadata": {},
   "source": [
    "**K-Fold cross-valuidation of Naive Bayes model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2295dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying k-Fold Cross Validation\n",
    "accuracies_nbc = cross_val_score(estimator=nbc, X=X_train, y=y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b10a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_nbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150b1282",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_nbc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e432a04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_nbc.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0a191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ['Random Forest', 'SVM', 'Naive Bayes']\n",
    "scores = [accuracies_rfc.mean(),accuracies_svc.mean(),accuracies_nbc.mean()]\n",
    "\n",
    "summary = pd.DataFrame(data=scores, index=model, columns=['Mean Accuracy'])\n",
    "summary.sort_values(by='Mean Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0268736b",
   "metadata": {},
   "source": [
    "# Conclusioni, il miglior modello risulta essere lo Structure Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = ['Random Forest', 'SVM', 'Naive Bayes']\n",
    "scores = [accuracies_rfc.mean(), accuracies_svc.mean(), accuracies_nbc.mean()]\n",
    "\n",
    "colors = ['#960018', '#F1C40F', '#960018'] # Bordeaux e Giallo oro della Roma\n",
    "plt.bar(model, scores, color=colors)\n",
    "plt.ylim([0, 1])\n",
    "plt.title('Accuratezza media dei modelli')\n",
    "plt.xlabel('Modello')\n",
    "plt.ylabel('Accuratezza media')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ba97ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
